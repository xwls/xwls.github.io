---
date: '2026-01-12T09:15:00+08:00'
draft: false
title: Python æ•°æ®å¤„ç†å®æˆ˜æŠ€å·§
summary: ä½¿ç”¨ Pandasã€NumPy ç­‰å·¥å…·è¿›è¡Œé«˜æ•ˆæ•°æ®å¤„ç†çš„å®Œæ•´æŒ‡å—
tags:
- Python
- Pandas
- NumPy
- æ•°æ®åˆ†æ
- æ•°æ®å¤„ç†
categories:
- æŠ€æœ¯æ•™ç¨‹
- æ•°æ®ç§‘å­¦
---

# Python æ•°æ®å¤„ç†å®æˆ˜æŠ€å·§

> æŒæ¡ Python æ•°æ®å¤„ç†ï¼Œè®©æ•°æ®åˆ†æå·¥ä½œäº‹åŠåŠŸå€

## æ¦‚è¿°

æœ¬æ–‡å°†ä»‹ç»ä½¿ç”¨ Python è¿›è¡Œæ•°æ®å¤„ç†çš„æ ¸å¿ƒæŠ€å·§ï¼ŒåŒ…æ‹¬ï¼š

- Pandas æ•°æ®æ“ä½œ
- NumPy æ•°å€¼è®¡ç®—
- æ•°æ®æ¸…æ´—ä¸è½¬æ¢
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§

> "æ•°æ®æ˜¯æ–°çš„çŸ³æ²¹ï¼Œä½†å¦‚æœä¸ç²¾ç‚¼ï¼Œå®ƒå°±æ¯«æ— ç”¨å¤„ã€‚" â€”â€” *Clive Humby*

---

## ç¯å¢ƒå‡†å¤‡

### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ venv
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# æˆ–ä½¿ç”¨ conda
conda create -n data-env python=3.11
conda activate data-env
```

### å®‰è£…ä¾èµ–

åˆ›å»º `requirements.txt`ï¼š

```txt
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
openpyxl>=3.1.0
jupyter>=1.0.0
```

å®‰è£…ï¼š

```bash
pip install -r requirements.txt
```

---

## Pandas åŸºç¡€

### æ•°æ®ç»“æ„

Pandas æœ‰ä¸¤ç§æ ¸å¿ƒæ•°æ®ç»“æ„ï¼š

| ç»“æ„ | ç»´åº¦ | æè¿° | ç±»æ¯” |
|:----:|:----:|:-----|:-----|
| `Series` | 1D | å¸¦æ ‡ç­¾çš„ä¸€ç»´æ•°ç»„ | Excel åˆ— |
| `DataFrame` | 2D | å¸¦æ ‡ç­¾çš„äºŒç»´è¡¨æ ¼ | Excel è¡¨ |

### åˆ›å»º DataFrame

```python
import pandas as pd
import numpy as np

# æ–¹å¼ 1ï¼šä»å­—å…¸åˆ›å»º
data = {
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
    'å¹´é¾„': [25, 30, 35, 28],
    'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'],
    'è–ªèµ„': [15000, 25000, 20000, 18000]
}
df = pd.DataFrame(data)

# æ–¹å¼ 2ï¼šä»åˆ—è¡¨åˆ›å»º
data_list = [
    ['å¼ ä¸‰', 25, 'åŒ—äº¬'],
    ['æå››', 30, 'ä¸Šæµ·'],
    ['ç‹äº”', 35, 'å¹¿å·']
]
df2 = pd.DataFrame(data_list, columns=['å§“å', 'å¹´é¾„', 'åŸå¸‚'])

# æ–¹å¼ 3ï¼šä» NumPy æ•°ç»„åˆ›å»º
arr = np.random.randn(5, 3)
df3 = pd.DataFrame(arr, columns=['A', 'B', 'C'])
```

è¾“å‡ºç¤ºä¾‹ï¼š

```
   å§“å  å¹´é¾„  åŸå¸‚    è–ªèµ„
0  å¼ ä¸‰   25  åŒ—äº¬  15000
1  æå››   30  ä¸Šæµ·  25000
2  ç‹äº”   35  å¹¿å·  20000
3  èµµå…­   28  æ·±åœ³  18000
```

### è¯»å–æ•°æ®

```python
# CSV æ–‡ä»¶
df = pd.read_csv('data.csv', encoding='utf-8')
df = pd.read_csv('data.csv', 
                 sep=',',           # åˆ†éš”ç¬¦
                 header=0,          # è¡¨å¤´è¡Œ
                 index_col='id',    # ç´¢å¼•åˆ—
                 usecols=['col1', 'col2'],  # åªè¯»å–æŒ‡å®šåˆ—
                 dtype={'col1': str},       # æŒ‡å®šæ•°æ®ç±»å‹
                 parse_dates=['date'],      # è§£ææ—¥æœŸåˆ—
                 nrows=1000)        # åªè¯»å–å‰ 1000 è¡Œ

# Excel æ–‡ä»¶
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df = pd.read_excel('data.xlsx', sheet_name=0)  # æŒ‰ç´¢å¼•

# JSON æ–‡ä»¶
df = pd.read_json('data.json')
df = pd.read_json('data.json', orient='records')

# SQL æ•°æ®åº“
from sqlalchemy import create_engine
engine = create_engine('postgresql://user:pass@localhost/db')
df = pd.read_sql('SELECT * FROM users', engine)
df = pd.read_sql_table('users', engine)
```

### æ•°æ®æ¢ç´¢

```python
# åŸºæœ¬ä¿¡æ¯
df.info()           # æ•°æ®ç±»å‹ã€éç©ºå€¼æ•°é‡
df.describe()       # ç»Ÿè®¡æ‘˜è¦
df.shape            # (è¡Œæ•°, åˆ—æ•°)
df.dtypes           # å„åˆ—æ•°æ®ç±»å‹
df.columns          # åˆ—ååˆ—è¡¨
df.index            # ç´¢å¼•

# æŸ¥çœ‹æ•°æ®
df.head(10)         # å‰ 10 è¡Œ
df.tail(5)          # å 5 è¡Œ
df.sample(5)        # éšæœº 5 è¡Œ

# å”¯ä¸€å€¼
df['åŸå¸‚'].unique()           # å”¯ä¸€å€¼æ•°ç»„
df['åŸå¸‚'].nunique()          # å”¯ä¸€å€¼æ•°é‡
df['åŸå¸‚'].value_counts()     # å„å€¼è®¡æ•°
```

---

## æ•°æ®é€‰æ‹©ä¸è¿‡æ»¤

### åˆ—é€‰æ‹©

```python
# å•åˆ—ï¼ˆè¿”å› Seriesï¼‰
df['å§“å']
df.å§“å  # å±æ€§è®¿é—®ï¼ˆåˆ—åæ— ç©ºæ ¼æ—¶å¯ç”¨ï¼‰

# å¤šåˆ—ï¼ˆè¿”å› DataFrameï¼‰
df[['å§“å', 'å¹´é¾„']]

# ä½¿ç”¨ locï¼ˆæ ‡ç­¾ï¼‰å’Œ ilocï¼ˆä½ç½®ï¼‰
df.loc[:, 'å§“å']           # æ‰€æœ‰è¡Œï¼Œå§“ååˆ—
df.iloc[:, 0]               # æ‰€æœ‰è¡Œï¼Œç¬¬ä¸€åˆ—
df.loc[:, 'å§“å':'åŸå¸‚']    # å§“ååˆ°åŸå¸‚çš„æ‰€æœ‰åˆ—
```

### è¡Œé€‰æ‹©

```python
# ä½¿ç”¨ locï¼ˆæ ‡ç­¾ç´¢å¼•ï¼‰
df.loc[0]                   # ç¬¬ä¸€è¡Œ
df.loc[0:2]                 # ç¬¬ 0-2 è¡Œï¼ˆåŒ…å« 2ï¼‰
df.loc[[0, 2, 4]]           # æŒ‡å®šè¡Œ

# ä½¿ç”¨ ilocï¼ˆæ•´æ•°ä½ç½®ç´¢å¼•ï¼‰
df.iloc[0]                  # ç¬¬ä¸€è¡Œ
df.iloc[0:2]                # ç¬¬ 0-1 è¡Œï¼ˆä¸åŒ…å« 2ï¼‰
df.iloc[[0, 2, 4]]          # æŒ‡å®šè¡Œ
```

### æ¡ä»¶è¿‡æ»¤

```python
# å•æ¡ä»¶
df[df['å¹´é¾„'] > 25]
df[df['åŸå¸‚'] == 'åŒ—äº¬']
df[df['å§“å'].str.contains('å¼ ')]

# å¤šæ¡ä»¶ï¼ˆä½¿ç”¨ & å’Œ |ï¼Œæ³¨æ„æ‹¬å·ï¼‰
df[(df['å¹´é¾„'] > 25) & (df['è–ªèµ„'] > 18000)]
df[(df['åŸå¸‚'] == 'åŒ—äº¬') | (df['åŸå¸‚'] == 'ä¸Šæµ·')]

# ä½¿ç”¨ isin
df[df['åŸå¸‚'].isin(['åŒ—äº¬', 'ä¸Šæµ·'])]

# ä½¿ç”¨ queryï¼ˆæ›´ç®€æ´ï¼‰
df.query('å¹´é¾„ > 25 and è–ªèµ„ > 18000')
df.query('åŸå¸‚ in ["åŒ—äº¬", "ä¸Šæµ·"]')

# ä½¿ç”¨ between
df[df['å¹´é¾„'].between(25, 35)]
```

---

## æ•°æ®æ¸…æ´—

### å¤„ç†ç¼ºå¤±å€¼

```python
# æ£€æŸ¥ç¼ºå¤±å€¼
df.isnull()                 # è¿”å›å¸ƒå°” DataFrame
df.isnull().sum()           # å„åˆ—ç¼ºå¤±å€¼æ•°é‡
df.isnull().sum().sum()     # æ€»ç¼ºå¤±å€¼æ•°é‡

# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()                 # åˆ é™¤ä»»ä½•åŒ…å« NaN çš„è¡Œ
df.dropna(how='all')        # åˆ é™¤å…¨ä¸º NaN çš„è¡Œ
df.dropna(subset=['å§“å'])  # åªæ£€æŸ¥æŒ‡å®šåˆ—
df.dropna(thresh=3)         # ä¿ç•™è‡³å°‘æœ‰ 3 ä¸ªéç©ºå€¼çš„è¡Œ

# å¡«å……ç¼ºå¤±å€¼
df.fillna(0)                # ç”¨ 0 å¡«å……
df.fillna(method='ffill')   # å‰å‘å¡«å……
df.fillna(method='bfill')   # åå‘å¡«å……
df['å¹´é¾„'].fillna(df['å¹´é¾„'].mean())  # ç”¨å‡å€¼å¡«å……

# æ’å€¼
df.interpolate(method='linear')  # çº¿æ€§æ’å€¼
```

### å¤„ç†é‡å¤å€¼

```python
# æ£€æŸ¥é‡å¤
df.duplicated()             # è¿”å›å¸ƒå°” Series
df.duplicated().sum()       # é‡å¤è¡Œæ•°é‡

# åˆ é™¤é‡å¤
df.drop_duplicates()                    # åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ
df.drop_duplicates(subset=['å§“å'])     # åŸºäºæŒ‡å®šåˆ—å»é‡
df.drop_duplicates(keep='first')        # ä¿ç•™ç¬¬ä¸€ä¸ª
df.drop_duplicates(keep='last')         # ä¿ç•™æœ€åä¸€ä¸ª
df.drop_duplicates(keep=False)          # åˆ é™¤æ‰€æœ‰é‡å¤
```

### æ•°æ®ç±»å‹è½¬æ¢

```python
# è½¬æ¢å•åˆ—
df['å¹´é¾„'] = df['å¹´é¾„'].astype(int)
df['è–ªèµ„'] = df['è–ªèµ„'].astype(float)
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])

# è½¬æ¢å¤šåˆ—
df = df.astype({
    'å¹´é¾„': 'int32',
    'è–ªèµ„': 'float64',
    'åŸå¸‚': 'category'  # åˆ†ç±»ç±»å‹ï¼ŒèŠ‚çœå†…å­˜
})

# å­—ç¬¦ä¸²è½¬æ•°å€¼ï¼ˆå¤„ç†é”™è¯¯ï¼‰
df['æ•°é‡'] = pd.to_numeric(df['æ•°é‡'], errors='coerce')  # æ— æ•ˆå€¼å˜ NaN
```

### å­—ç¬¦ä¸²å¤„ç†

```python
# å­—ç¬¦ä¸²æ–¹æ³•ï¼ˆé€šè¿‡ .str è®¿é—®å™¨ï¼‰
df['å§“å'].str.lower()          # å°å†™
df['å§“å'].str.upper()          # å¤§å†™
df['å§“å'].str.strip()          # å»é™¤ç©ºç™½
df['å§“å'].str.replace('è€', 'å°')  # æ›¿æ¢
df['å§“å'].str.len()            # é•¿åº¦

# åˆ†å‰²ä¸åˆå¹¶
df['å§“å'].str.split(' ')       # åˆ†å‰²ä¸ºåˆ—è¡¨
df['å§“å'].str.split(' ', expand=True)  # åˆ†å‰²ä¸ºå¤šåˆ—

# æ­£åˆ™è¡¨è¾¾å¼
df['é‚®ç®±'].str.extract(r'(\w+)@(\w+)\.com')  # æå–
df['æ‰‹æœº'].str.match(r'1[3-9]\d{9}')         # åŒ¹é…
```

---

## æ•°æ®è½¬æ¢

### æ·»åŠ ä¸ä¿®æ”¹åˆ—

```python
# ç›´æ¥èµ‹å€¼
df['æ–°åˆ—'] = 0
df['å¹´è–ª'] = df['è–ªèµ„'] * 12

# ä½¿ç”¨ assignï¼ˆé“¾å¼æ“ä½œï¼‰
df = df.assign(
    å¹´è–ª=lambda x: x['è–ªèµ„'] * 12,
    ç¨å=lambda x: x['å¹´è–ª'] * 0.8
)

# æ¡ä»¶èµ‹å€¼
df['çº§åˆ«'] = np.where(df['è–ªèµ„'] > 20000, 'é«˜çº§', 'æ™®é€š')

# å¤šæ¡ä»¶èµ‹å€¼
conditions = [
    df['è–ªèµ„'] >= 25000,
    df['è–ªèµ„'] >= 18000,
    df['è–ªèµ„'] < 18000
]
choices = ['é«˜çº§', 'ä¸­çº§', 'åˆçº§']
df['çº§åˆ«'] = np.select(conditions, choices)

# ä½¿ç”¨ apply
df['å§“åé•¿åº¦'] = df['å§“å'].apply(len)
df['çº§åˆ«'] = df['è–ªèµ„'].apply(lambda x: 'é«˜çº§' if x > 20000 else 'æ™®é€š')
```

### é‡å‘½åä¸é‡æ’

```python
# é‡å‘½ååˆ—
df.rename(columns={'å§“å': 'name', 'å¹´é¾„': 'age'})
df.columns = ['name', 'age', 'city', 'salary']

# é‡æ’åˆ—é¡ºåº
df = df[['å§“å', 'åŸå¸‚', 'å¹´é¾„', 'è–ªèµ„']]
df = df.reindex(columns=['å§“å', 'åŸå¸‚', 'å¹´é¾„', 'è–ªèµ„'])

# æ’åº
df.sort_values('è–ªèµ„', ascending=False)           # é™åº
df.sort_values(['åŸå¸‚', 'è–ªèµ„'], ascending=[True, False])  # å¤šåˆ—
df.sort_index()  # æŒ‰ç´¢å¼•æ’åº
```

### åˆ†ç»„èšåˆ

```python
# åŸºæœ¬åˆ†ç»„
grouped = df.groupby('åŸå¸‚')
grouped['è–ªèµ„'].mean()          # å„åŸå¸‚å¹³å‡è–ªèµ„
grouped['è–ªèµ„'].agg(['mean', 'max', 'min', 'count'])

# å¤šåˆ—åˆ†ç»„
df.groupby(['åŸå¸‚', 'çº§åˆ«'])['è–ªèµ„'].mean()

# è‡ªå®šä¹‰èšåˆ
df.groupby('åŸå¸‚').agg({
    'è–ªèµ„': ['mean', 'sum'],
    'å¹´é¾„': 'max',
    'å§“å': 'count'
})

# ä½¿ç”¨ transformï¼ˆä¿æŒåŸå§‹è¡Œæ•°ï¼‰
df['åŸå¸‚å¹³å‡è–ªèµ„'] = df.groupby('åŸå¸‚')['è–ªèµ„'].transform('mean')
df['è–ªèµ„åå·®'] = df['è–ªèµ„'] - df['åŸå¸‚å¹³å‡è–ªèµ„']

# ä½¿ç”¨ applyï¼ˆæ›´çµæ´»ï¼‰
def top_n(group, n=2):
    return group.nlargest(n, 'è–ªèµ„')

df.groupby('åŸå¸‚').apply(top_n)
```

### æ•°æ®é€è§†è¡¨

```python
# pivot_table
pd.pivot_table(
    df,
    values='è–ªèµ„',
    index='åŸå¸‚',
    columns='çº§åˆ«',
    aggfunc='mean',
    fill_value=0,
    margins=True  # æ·»åŠ æ±‡æ€»è¡Œ/åˆ—
)

# ç¤ºä¾‹è¾“å‡ºï¼š
# çº§åˆ«      ä¸­çº§      åˆçº§      é«˜çº§       All
# åŸå¸‚
# ä¸Šæµ·   22000.0   16000.0   28000.0   22000.0
# åŒ—äº¬   20000.0   15000.0   25000.0   20000.0
# ...

# crosstabï¼ˆäº¤å‰è¡¨ï¼‰
pd.crosstab(df['åŸå¸‚'], df['çº§åˆ«'], margins=True)
```

---

## åˆå¹¶æ•°æ®

### concat æ‹¼æ¥

```python
# çºµå‘æ‹¼æ¥ï¼ˆè¡Œï¼‰
df_all = pd.concat([df1, df2, df3])
df_all = pd.concat([df1, df2], ignore_index=True)  # é‡ç½®ç´¢å¼•

# æ¨ªå‘æ‹¼æ¥ï¼ˆåˆ—ï¼‰
df_wide = pd.concat([df1, df2], axis=1)
```

### merge åˆå¹¶

```python
# ç±»ä¼¼ SQL JOIN
df_merged = pd.merge(df1, df2, on='id')  # å†…è¿æ¥
df_merged = pd.merge(df1, df2, on='id', how='left')   # å·¦è¿æ¥
df_merged = pd.merge(df1, df2, on='id', how='right')  # å³è¿æ¥
df_merged = pd.merge(df1, df2, on='id', how='outer')  # å…¨å¤–è¿æ¥

# å¤šé”®åˆå¹¶
df_merged = pd.merge(df1, df2, on=['id', 'date'])

# ä¸åŒåˆ—å
df_merged = pd.merge(df1, df2, left_on='user_id', right_on='id')
```

è¿æ¥ç±»å‹å¯¹æ¯”ï¼š

```
Left:  â”Œâ”€â”€â”€â”€â”€â”         Right: â”Œâ”€â”€â”€â”€â”€â”
       â”‚  A  â”‚                â”‚  B  â”‚
       â”‚  B  â”‚                â”‚  C  â”‚
       â”‚  C  â”‚                â”‚  D  â”‚
       â””â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”˜

Inner (äº¤é›†):  Left (å·¦å…¨): Right (å³å…¨): Outer (å¹¶é›†):
â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”
â”‚  B  â”‚       â”‚  A  â”‚      â”‚  B  â”‚       â”‚  A  â”‚
â”‚  C  â”‚       â”‚  B  â”‚      â”‚  C  â”‚       â”‚  B  â”‚
â””â”€â”€â”€â”€â”€â”˜       â”‚  C  â”‚      â”‚  D  â”‚       â”‚  C  â”‚
              â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜       â”‚  D  â”‚
                                         â””â”€â”€â”€â”€â”€â”˜
```

---

## NumPy é«˜æ•ˆè®¡ç®—

### æ•°ç»„åˆ›å»º

```python
import numpy as np

# åŸºæœ¬åˆ›å»º
arr = np.array([1, 2, 3, 4, 5])
arr_2d = np.array([[1, 2, 3], [4, 5, 6]])

# ç‰¹æ®Šæ•°ç»„
np.zeros((3, 4))          # å…¨ 0
np.ones((3, 4))           # å…¨ 1
np.full((3, 4), 7)        # å…¨ 7
np.eye(4)                 # å•ä½çŸ©é˜µ
np.empty((3, 4))          # æœªåˆå§‹åŒ–ï¼ˆå¿«é€Ÿï¼‰

# åºåˆ—
np.arange(0, 10, 2)       # [0, 2, 4, 6, 8]
np.linspace(0, 1, 5)      # [0, 0.25, 0.5, 0.75, 1]
np.logspace(0, 3, 4)      # [1, 10, 100, 1000]

# éšæœºæ•°
np.random.seed(42)        # è®¾ç½®ç§å­
np.random.rand(3, 4)      # å‡åŒ€åˆ†å¸ƒ [0, 1)
np.random.randn(3, 4)     # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
np.random.randint(0, 10, (3, 4))  # æ•´æ•°
np.random.choice([1, 2, 3], 10)   # éšæœºé€‰æ‹©
```

### æ•°ç»„æ“ä½œ

```python
# å½¢çŠ¶æ“ä½œ
arr.shape                 # å½¢çŠ¶
arr.reshape(2, 3)         # é‡å¡‘ï¼ˆä¸æ”¹å˜æ•°æ®ï¼‰
arr.ravel()               # å±•å¹³ä¸º 1D
arr.T                     # è½¬ç½®

# æ•°å­¦è¿ç®—ï¼ˆé€å…ƒç´ ï¼‰
arr + 1                   # åŠ æ³•
arr * 2                   # ä¹˜æ³•
arr ** 2                  # å¹‚
np.sqrt(arr)              # å¹³æ–¹æ ¹
np.exp(arr)               # æŒ‡æ•°
np.log(arr)               # å¯¹æ•°

# ç»Ÿè®¡å‡½æ•°
arr.sum()                 # æ±‚å’Œ
arr.mean()                # å‡å€¼
arr.std()                 # æ ‡å‡†å·®
arr.min(), arr.max()      # æœ€å€¼
arr.argmin(), arr.argmax()  # æœ€å€¼ç´¢å¼•

# æ²¿è½´æ“ä½œ
arr_2d.sum(axis=0)        # æŒ‰åˆ—æ±‚å’Œ
arr_2d.sum(axis=1)        # æŒ‰è¡Œæ±‚å’Œ
```

### å‘é‡åŒ–æ“ä½œ

```python
# é¿å…å¾ªç¯ï¼ä½¿ç”¨å‘é‡åŒ–

# âŒ æ…¢ï¼šPython å¾ªç¯
result = []
for x in arr:
    result.append(x ** 2 + 2 * x + 1)

# âœ… å¿«ï¼šNumPy å‘é‡åŒ–
result = arr ** 2 + 2 * arr + 1

# æ¡ä»¶æ“ä½œ
np.where(arr > 0, arr, 0)  # è´Ÿæ•°æ›¿æ¢ä¸º 0

# å¸ƒå°”ç´¢å¼•
arr[arr > 0]              # åªä¿ç•™æ­£æ•°
arr[(arr > 0) & (arr < 5)]  # ç»„åˆæ¡ä»¶
```

---

## æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–

```python
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
df.info(memory_usage='deep')
df.memory_usage(deep=True)

# ä¼˜åŒ–æ•°æ®ç±»å‹
def optimize_dtypes(df):
    """è‡ªåŠ¨ä¼˜åŒ– DataFrame æ•°æ®ç±»å‹"""
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'object':
            # å°è¯•è½¬æ¢ä¸º category
            if df[col].nunique() / len(df) < 0.5:
                df[col] = df[col].astype('category')
        
        elif col_type == 'int64':
            # ä½¿ç”¨æ›´å°çš„æ•´æ•°ç±»å‹
            if df[col].min() >= 0:
                if df[col].max() < 255:
                    df[col] = df[col].astype('uint8')
                elif df[col].max() < 65535:
                    df[col] = df[col].astype('uint16')
            else:
                if df[col].max() < 32767:
                    df[col] = df[col].astype('int16')
        
        elif col_type == 'float64':
            # ä½¿ç”¨ float32
            df[col] = df[col].astype('float32')
    
    return df

df_optimized = optimize_dtypes(df.copy())
```

### è®¡ç®—ä¼˜åŒ–

```python
# 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œä»£æ›¿ apply
# âŒ æ…¢
df['new'] = df['col'].apply(lambda x: x ** 2)
# âœ… å¿«
df['new'] = df['col'] ** 2

# 2. ä½¿ç”¨ eval å’Œ query
# âŒ æ…¢
df['result'] = df['A'] + df['B'] * df['C']
# âœ… å¿«ï¼ˆå¤§æ•°æ®é›†ï¼‰
df.eval('result = A + B * C', inplace=True)

# 3. åˆ†å—å¤„ç†å¤§æ–‡ä»¶
chunks = pd.read_csv('large_file.csv', chunksize=100000)
result = pd.concat([process(chunk) for chunk in chunks])

# 4. ä½¿ç”¨ Numba JIT ç¼–è¯‘
from numba import jit

@jit(nopython=True)
def fast_function(arr):
    result = 0
    for i in range(len(arr)):
        result += arr[i] ** 2
    return result
```

### å¹¶è¡Œå¤„ç†

```python
# ä½¿ç”¨ multiprocessing
from multiprocessing import Pool

def process_chunk(chunk):
    # å¤„ç†é€»è¾‘
    return chunk.apply(some_function)

with Pool(4) as pool:
    chunks = np.array_split(df, 4)
    results = pool.map(process_chunk, chunks)
df_result = pd.concat(results)

# ä½¿ç”¨ pandarallel
from pandarallel import pandarallel
pandarallel.initialize(progress_bar=True)

df['new'] = df['col'].parallel_apply(complex_function)
```

---

## å®æˆ˜æ¡ˆä¾‹

### é”€å”®æ•°æ®åˆ†æ

```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_csv('sales_data.csv', parse_dates=['date'])

# æ•°æ®æ¸…æ´—
df = df.dropna(subset=['amount'])
df['amount'] = df['amount'].abs()

# ç‰¹å¾å·¥ç¨‹
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['weekday'] = df['date'].dt.day_name()

# åˆ†æ
monthly_sales = df.groupby(['year', 'month'])['amount'].agg([
    ('æ€»é”€å”®é¢', 'sum'),
    ('è®¢å•æ•°', 'count'),
    ('å¹³å‡å®¢å•ä»·', 'mean')
]).round(2)

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# æœˆåº¦é”€å”®è¶‹åŠ¿
df.groupby(df['date'].dt.to_period('M'))['amount'].sum().plot(
    ax=axes[0, 0], 
    kind='line',
    title='æœˆåº¦é”€å”®è¶‹åŠ¿'
)

# äº§å“é”€å”®å æ¯”
df.groupby('product')['amount'].sum().plot(
    ax=axes[0, 1],
    kind='pie',
    autopct='%1.1f%%',
    title='äº§å“é”€å”®å æ¯”'
)

# æ˜ŸæœŸåˆ†å¸ƒ
df.groupby('weekday')['amount'].mean().reindex([
    'Monday', 'Tuesday', 'Wednesday', 
    'Thursday', 'Friday', 'Saturday', 'Sunday'
]).plot(ax=axes[1, 0], kind='bar', title='æ¯å‘¨é”€å”®åˆ†å¸ƒ')

# é”€å”®é¢åˆ†å¸ƒ
df['amount'].hist(ax=axes[1, 1], bins=50, edgecolor='black')
axes[1, 1].set_title('é”€å”®é¢åˆ†å¸ƒ')

plt.tight_layout()
plt.savefig('sales_analysis.png', dpi=150)
plt.show()
```

---

## å¸¸è§é—®é¢˜

<details>
<summary><strong>Q: SettingWithCopyWarning æ˜¯ä»€ä¹ˆï¼Ÿ</strong></summary>

è¿™ä¸ªè­¦å‘Šè¡¨ç¤ºä½ å¯èƒ½åœ¨ä¿®æ”¹ä¸€ä¸ªåˆ‡ç‰‡çš„å‰¯æœ¬ï¼Œè€Œä¸æ˜¯åŸå§‹ DataFrameã€‚

```python
# âŒ å¯èƒ½è§¦å‘è­¦å‘Š
df[df['A'] > 0]['B'] = 1

# âœ… æ­£ç¡®åšæ³•
df.loc[df['A'] > 0, 'B'] = 1
```
</details>

<details>
<summary><strong>Q: å¦‚ä½•å¤„ç†å¤§å‹ CSV æ–‡ä»¶ï¼Ÿ</strong></summary>

```python
# åˆ†å—è¯»å–
for chunk in pd.read_csv('large.csv', chunksize=100000):
    process(chunk)

# åªè¯»å–éœ€è¦çš„åˆ—
df = pd.read_csv('large.csv', usecols=['col1', 'col2'])

# æŒ‡å®šæ•°æ®ç±»å‹å‡å°‘å†…å­˜
df = pd.read_csv('large.csv', dtype={'id': 'int32', 'value': 'float32'})
```
</details>

<details>
<summary><strong>Q: apply å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ</strong></summary>

1. ä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–æ“ä½œ
2. ä½¿ç”¨ `map` ä»£æ›¿ç®€å•çš„ `apply`
3. ä½¿ç”¨ Numba JIT ç¼–è¯‘
4. ä½¿ç”¨ `pandarallel` å¹¶è¡Œå¤„ç†
</details>

---

## æ€»ç»“

æœ¬æ–‡ä»‹ç»äº† Python æ•°æ®å¤„ç†çš„æ ¸å¿ƒçŸ¥è¯†ï¼š

| ä¸»é¢˜ | å…³é”®æŠ€æœ¯ |
|------|----------|
| æ•°æ®è¯»å– | `read_csv`, `read_excel`, `read_sql` |
| æ•°æ®é€‰æ‹© | `loc`, `iloc`, æ¡ä»¶è¿‡æ»¤, `query` |
| æ•°æ®æ¸…æ´— | `dropna`, `fillna`, `drop_duplicates` |
| æ•°æ®è½¬æ¢ | `apply`, `assign`, `groupby`, `merge` |
| æ€§èƒ½ä¼˜åŒ– | æ•°æ®ç±»å‹ä¼˜åŒ–, å‘é‡åŒ–, å¹¶è¡Œå¤„ç† |

> ğŸ’¡ **è®°ä½**ï¼š*å‘é‡åŒ–ä¼˜å…ˆï¼Œé¿å…å¾ªç¯ï¼*

---

## å‚è€ƒèµ„æ–™

1. [Pandas å®˜æ–¹æ–‡æ¡£](https://pandas.pydata.org/docs/)
2. [NumPy å®˜æ–¹æ–‡æ¡£](https://numpy.org/doc/)
3. [Python for Data Analysis](https://wesmckinney.com/book/) - Wes McKinney
4. [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)

[^1]: Pandas 2.0 å¼•å…¥äº† PyArrow åç«¯ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

---

*æœ¬æ–‡æœ€åæ›´æ–°äº 2026 å¹´ 1 æœˆ*